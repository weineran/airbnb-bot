{"paragraphs":[{"text":"/* Import dependencies */\nimport com.mongodb.spark._\nimport com.mongodb.spark.config._\nimport org.bson.Document\n\n/* Create a sequence of json-formatted strings */\nval docs = \"\"\"\n  {\"name\": \"Bilbo Baggins\", \"age\": 50}\n  {\"name\": \"Gandalf\", \"age\": 1000}\n  {\"name\": \"Thorin\", \"age\": 195}\n  {\"name\": \"Balin\", \"age\": 178}\n  {\"name\": \"Kíli\", \"age\": 77}\n  {\"name\": \"Dwalin\", \"age\": 169}\n  {\"name\": \"Óin\", \"age\": 167}\n  {\"name\": \"Glóin\", \"age\": 158}\n  {\"name\": \"Fíli\", \"age\": 82}\n  {\"name\": \"Bombur\"}\"\"\".trim.stripMargin.split(\"[\\\\r\\\\n]+\").toSeq\n\n/* parse the json-formatted strings as MongoDB document */\nval documents = sc.parallelize(docs.map(Document.parse))\n\n/**\n * NOTE TO USER: In your terminal, run \"docker ps\" to see the names of your currently running docker containers.\n * Find the name of the MongoDB container and paste it into the string below.\n */\nval mongoDbContainerName = \"airbnbbot_mongodb_1\" // <- PASTE YOUR MONGODB CONTAINER NAME HERE\nval dbName = \"myDB\"\nval collectionName = \"myCollection\"\nval mongoDbUri = \"mongodb://\" + mongoDbContainerName + \"/\" + dbName + \".\" + collectionName\n\n/* Save documents to MongoDB (the one in the docker container) */\nprintln(\"Saving data to \" + mongoDbUri)\ndocuments.saveToMongoDB(WriteConfig(Map(\"uri\" -> mongoDbUri)))\nprintln(\"Done.\")\n","dateUpdated":"2017-06-04T21:19:58+0000","config":{"enabled":true,"results":{},"editorMode":"ace/mode/scala","editorSetting":{"language":"scala"},"colWidth":12},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport com.mongodb.spark._\n\nimport com.mongodb.spark.config._\n\n\nimport org.bson.Document\ndocs: Seq[String] = WrappedArray({\"name\": \"Bilbo Baggins\", \"age\": 50}, \"  {\"name\": \"Gandalf\", \"age\": 1000}\", \"  {\"name\": \"Thorin\", \"age\": 195}\", \"  {\"name\": \"Balin\", \"age\": 178}\", \"  {\"name\": \"Kíli\", \"age\": 77}\", \"  {\"name\": \"Dwalin\", \"age\": 169}\", \"  {\"name\": \"Óin\", \"age\": 167}\", \"  {\"name\": \"Glóin\", \"age\": 158}\", \"  {\"name\": \"Fíli\", \"age\": 82}\", \"  {\"name\": \"Bombur\"}\")\n\ndocuments: org.apache.spark.rdd.RDD[org.bson.Document] = ParallelCollectionRDD[98] at parallelize at <console>:163\n\nmongoDbContainerName: String = airbnbbot_mongodb_1\n\ndbName: String = myDB\n\ncollectionName: String = myCollection\nSaving data to mongodb://airbnbbot_mongodb_1/myDB.myCollection\nDone.\n\nmongoDbUri: String = mongodb://airbnbbot_mongodb_1/myDB.myCollection\n"}]},"apps":[],"jobName":"paragraph_1496600471333_-644977479","id":"20170604-172329_1884401710","dateCreated":"2017-06-04T18:21:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5374","user":"anonymous","dateFinished":"2017-06-04T21:20:03+0000","dateStarted":"2017-06-04T21:19:58+0000"},{"dateUpdated":"2017-06-04T21:20:07+0000","config":{"enabled":true,"results":{},"editorMode":"ace/mode/scala","editorSetting":{"language":"scala"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1496600471334_-643823233","id":"20170604-172340_1122904255","dateCreated":"2017-06-04T18:21:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5375","text":"/* Get the collection as a Spark dataframe */\nval df = sc.loadFromMongoDB(ReadConfig(Map(\"uri\" -> mongoDbUri))).toDF()\n\n/* The schema is automatically inferred.  Have a look. */\ndf.printSchema()\n\n/* Show the data */\ndf.show()\n","user":"anonymous","dateFinished":"2017-06-04T21:20:09+0000","dateStarted":"2017-06-04T21:20:07+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- _id: struct (nullable = true)\n |    |-- oid: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- name: string (nullable = true)\n\n\ndf: org.apache.spark.sql.DataFrame = [_id: struct<oid: string>, age: int ... 1 more field]\n+--------------------+----+-------------+\n|                 _id| age|         name|\n+--------------------+----+-------------+\n|[59347983857aba00...|  50|Bilbo Baggins|\n|[59347983857aba00...| 169|       Dwalin|\n|[59347983857aba00...| 195|       Thorin|\n|[59347983857aba00...| 178|        Balin|\n|[59347983857aba00...|1000|      Gandalf|\n|[59347983857aba00...| 167|          Óin|\n|[59347983857aba00...|  77|         Kíli|\n|[59347983857aba00...| 158|        Glóin|\n|[59347983857aba00...|  82|         Fíli|\n|[59347983857aba00...|null|       Bombur|\n+--------------------+----+-------------+\n\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1496607385686_1839383845","id":"20170604-201625_532357494","dateCreated":"2017-06-04T20:16:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5685","text":"/* We can run SQL queries on the collection */\n\n/* Register the dataframe as a temporary table */\ndf.registerTempTable(\"characters\")\n\n/* Run a query and show results */\nval centenarians = spark.sql(\"SELECT name, age FROM characters WHERE age >= 100\")\ncentenarians.show()\n","dateUpdated":"2017-06-04T21:20:14+0000","dateFinished":"2017-06-04T21:20:16+0000","dateStarted":"2017-06-04T21:20:15+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\ncentenarians: org.apache.spark.sql.DataFrame = [name: string, age: int]\n+-------+----+\n|   name| age|\n+-------+----+\n| Dwalin| 169|\n| Thorin| 195|\n|  Balin| 178|\n|Gandalf|1000|\n|    Óin| 167|\n|  Glóin| 158|\n+-------+----+\n\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sql","editOnDblClick":false},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1496610497460_-1727746922","id":"20170604-210817_2070318329","dateCreated":"2017-06-04T21:08:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6193","text":"%sql\n\n/* Another way to run the same query */\nSELECT name, age\nFROM characters\nWHERE age >= 100","dateUpdated":"2017-06-04T21:20:21+0000","dateFinished":"2017-06-04T21:20:21+0000","dateStarted":"2017-06-04T21:20:21+0000","results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tage\nDwalin\t169\nThorin\t195\nBalin\t178\nGandalf\t1000\nÓin\t167\nGlóin\t158\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1496610835345_338994474","id":"20170604-211355_333538957","dateCreated":"2017-06-04T21:13:55+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6370"}],"name":"Spark Connector Tutorial","id":"2CMJT795C","angularObjects":{"2CMTV4GHP:shared_process":[],"2CHD2BEBB:shared_process":[],"2CJK5A79M:shared_process":[],"2CMA92VRG:shared_process":[],"2CMSYEW4C:shared_process":[],"2CM7QPRVG:shared_process":[],"2CGU5AS6K:shared_process":[],"2CJCQJKT9:shared_process":[],"2CJUZZFH6:shared_process":[],"2CKT6ZPKN:shared_process":[],"2CH76UXRV:shared_process":[],"2CJC2PAJ7:shared_process":[],"2CJNMGZWN:shared_process":[],"2CKGTBH84:shared_process":[],"2CJH536CK:shared_process":[],"2CH5N83QZ:shared_process":[],"2CKRACCYM:shared_process":[],"2CKJV8ERR:shared_process":[],"2CHNUPSX5:shared_process":[],"2CK75FN8E:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}